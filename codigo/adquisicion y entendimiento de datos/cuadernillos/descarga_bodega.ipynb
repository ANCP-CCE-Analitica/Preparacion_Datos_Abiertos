{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  padding: 30px;\n",
    "  text-align: center;\" class='row'>\n",
    "<div style=\"float:left;width: 15%;\" class='column'><a href=\"https://www.colombiacompra.gov.co\"><img alt=\"Logo Colombia Compra Eficiente\" id=\"logocce\" src=\"https://www.colombiacompra.gov.co/sites/cce_public/files/Images/logocce.png\" style=\"height: 45px;\"></a></div>\n",
    "    <div style=\"float:left;width: 70%;\" class='column'>\n",
    "        <h1> Exploración Datalake\n",
    "        </h1> \n",
    "    </div>\n",
    "<div style=\"float:left;width: 15%;\" class='column'><a href=\"https://www.dnp.gov.co/\" target=\"_blank\"><img class=\"float-right\" id=\"logodnp\" src=\"https://www.dnp.gov.co/img/logoNuevo.jpg\" style=\"width: 200px;\"></a></div>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IDENTIFICACIÓN DEL INSUMO\n",
    "\n",
    "|||\n",
    "|:--|:--|\n",
    "|**Fecha**|Abril 2023|\n",
    "|**Ciudad**|Bogotá D.C.|\n",
    "|**Esquema de presentación del insumo**|Cuaderno Jupyter|\n",
    "|**Título del insumo**| **Consideraciones para la consulta en datos abiertos**|\n",
    "|**Descripción y alcance**|Script para la consulta de bases y exploración de bases en el DATAVAULT.|\n",
    "|**Periodicidad del insumo**|único|\n",
    "|**Solicitante**|No aplica|\n",
    "|**Versión del insumo**|Final|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DESTINO Y AUTORES DEL INFORME / INSUMO\n",
    "\n",
    "|||\n",
    "|:--|:--|\n",
    "|**Destinatario**|<table align='left'><tr><td>*Nombre:*</td> <td>Equipo analítica EMAE</td></tr> <tr><td>*Cargo:*</td> <td>NA</td></tr>  <tr><td>*Área:*</td> <td>Subdirección de estudios de Mercado y Abastecimiento Estratégico – EMAE</td></tr></table>|\n",
    "|**Autores**|<table><tr><td>*Nombre:*</td> <td>Equipo de Datos -GAEC<td>*Área:*</td> <td>Subdirección de estudios de Mercado y Abastecimiento Estratégico – EMAE.</td></tr></table>|\n",
    "|**Aprobación**|<table><tr><td>*Nombre:*</td> <td>Ricardo Suarez</td></tr> <tr><td>*Cargo:*</td> <td>Subdirector Estudios de Mercado y Abastecimiento Estratégico</td></tr>  <tr><td>*Área:*</td> <td>Subdirección de estudios de Mercado y Abastecimiento Estratégico – EMAE.</td></tr></table>|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../../../muestras%20de%20datos/sin%20procesar/descarga_bodega_.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaciones previas\n",
    " \n",
    "Para poder utilizar SQL en Python es necesario instalar lo siguiente, escoja el driver de acuerdo al sistema operativo de su equipo: \n",
    " \n",
    " \n",
    "**Driver Microsoft ODBC para SQL SERVER en windows**\n",
    " \n",
    "Descargar e instalar el driver de la página [Microsoft ODBC](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/system-requirements-installation-and-driver-files?view=sql-server-ver15#installing-microsoft-odbc-driver-for-sql-server) versión \n",
    " 17.\n",
    " \n",
    "**Driver Microsoft ODBC para SQL SERVER en Linux o MacOs**\n",
    " \n",
    "Descargar e instalar el driver de la página [Microsoft ODBC](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/system-requirements?view=sql-server-ver15) versión 17 deacuerdo a la versión de su sistema operativo.\n",
    "Recuerde estar conectado a la VPN para poder accerder. \n",
    "\n",
    "**Paquete pyobdc**\n",
    "\n",
    "Para instalar el paquete pyodbc ejecute el siguiente comando en la consola de su equipo:\n",
    "\n",
    "```python\n",
    "pip install pyodbc\n",
    "```\n",
    "\n",
    "**Paquete pandas**\n",
    "\n",
    "Se sugiere una versión mayor que la de 2.0.1 si no tiene instalado pandas ejecute el siguiente comando en la consola de su equipo:\n",
    "\n",
    "```python\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "Si lo tiene pero desactualizado ejecute el siguiente comando en la consola de su equipo:\n",
    "\n",
    "```python\n",
    "pip install --upgrade pandas\n",
    "```\n",
    "\n",
    "Para este ejercicio también es necesario instalar el paquete pyarrow, para ello ejecute el siguiente comando en la consola de su equipo:\n",
    "\n",
    "```python\n",
    "pip install pyarrow\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "server_reports= os.getenv(\"server_reports\")\n",
    "server_sandbox= os.getenv(\"server_sandbox\")\n",
    "username= os.getenv(\"username\")\n",
    "password= os.getenv(\"password\")\n",
    "\n",
    "cnxn_reports = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server_reports+';UID='+username+';PWD='+ password)\n",
    "cursor_reports = cnxn_reports.cursor()\n",
    "\n",
    "cnxn_sandbox = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server_sandbox+';UID='+username+';PWD='+ password)\n",
    "cursor_sandbox = cnxn_sandbox.cursor()\n",
    "\n",
    "\n",
    "ruta_DATA=\"../../../../DATA/\"\n",
    "ruta_MUESTRAS=\"../../../muestras de datos/sin procesar/MUESTRAS/ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paquetes usados para la exploración de datos\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from scripts import data_management as dm # Funciones definidas\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tablas Requeridas\n",
    "\n",
    "En particular de aqui saldran las siguientes tablas para poner a disposición del datalake de desarrollo del equipo de datos de EMAE:\n",
    "\n",
    "\n",
    "\n",
    "- **Tabla de procesos SECOP II**: Contiene la información de los procesos que se han llevado a cabo en el Sistema Electrónico de Contratación Pública SECOP II.\n",
    "\n",
    "- **Tabla de contratos SECOP II**: Contiene la información de los contratos que se han llevado a cabo en el Sistema Electrónico de Contratación Pública SECOP II.\n",
    "\n",
    "\n",
    "- **Tabla de procesos SECOP I**: Contiene la información de los procesos que se han llevado a cabo en el Sistema Electrónico de Contratación Pública SECOP I.\n",
    "\n",
    "- **Tablas TVEC**: Contienen la información de la tienda virtual del estado colombiano.\n",
    "\n",
    "El equipo EMAE procesará los datos desde el datalake de desarrollo, particularmente tomando los archivos en las carpetas bronze y posteriormente dejará todos sus resultados en las carpetas silver y gold. \n",
    "\n",
    "![image.png](../../../muestras%20de%20datos/sin%20procesar/Ingesta%20Bodega%20a%20DL.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ADQUISICIÓN DE LOS DATOS\n",
    "\n",
    "### 4.1. Datos de procesos SECOP II\n",
    "\n",
    "Para consultar los datos de procesos SECOP II, se debe ejecutar la siguiente consulta:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM [CCE_Sandbox].[datavault].[SAT_Example_Table]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Consulta de contratos depurados\n",
    "# n=pd.read_sql_query(\"SELECT COUNT(*) FROM [CCE_Reports].[SECOPII].[V_HistoricoContratos_Depurado]\", cnxn).values[0][0]\n",
    "# Lista_Contratos_Depurado=[]\n",
    "# for i in range(0,n,1000000):\n",
    "#     print(\"Inicia la consulta en\", i)\n",
    "#     SECOP_II_Contratos_Depurado_temp = pd.read_sql_query(\"SELECT * FROM [CCE_Reports].[SECOPII].[V_HistoricoContratos_Depurado] ORDER BY [Nombre Entidad] OFFSET \"+str(i)+\" ROWS FETCH NEXT 1000000 ROWS ONLY\", cnxn)\n",
    "#     Lista_Contratos_Depurado.append(SECOP_II_Contratos_Depurado_temp)\n",
    "\n",
    "# SECOP_II_Contratos_Depurado=pd.concat(Lista_Contratos_Depurado)\n",
    "\n",
    "\n",
    "\n",
    "# ## Ruta de secop II contratos depurados\n",
    "# ruta_Large=\"/dropbox/Dropbox/CCE-projects/Large Data/\"\n",
    "# SECOP_II_Contratos_Depurado.to_parquet(ruta_Large+\"SECOP_II_Contratos_Depurado.parquet\", engine='pyarrow')\n",
    "\n",
    "\n",
    "# ## Consulta de SECOP II procedimientos depurados\n",
    "# n=pd.read_sql_query(\"SELECT COUNT(*) FROM [CCE_Reports].[SECOPII].[V_HistoricoProcedimientos_Depurado]\", cnxn).values[0][0]\n",
    "# Lista_Procesos_Depurado=[]\n",
    "# for i in range(0,n,100000):\n",
    "#     print(\"Inicia la consulta en\", i)\n",
    "#     SECOP_Procesos_Depurado_temp = pd.read_sql_query(\"SELECT * FROM [CCE_Reports].[SECOPII].[V_HistoricoProcedimientos_Depurado] ORDER BY [Nombre Entidad] OFFSET \"+str(i)+\" ROWS FETCH NEXT 100000 ROWS ONLY\", cnxn)\n",
    "#     Lista_Procesos_Depurado.append(SECOP_Procesos_Depurado_temp)\n",
    "\n",
    "# SECOP_II_Procesos_Depurado=pd.concat(Lista_Procesos_Depurado)\n",
    "# SECOP_II_Procesos_Depurado = pd.read_sql_query(\"SELECT * FROM [CCE_Reports].[SECOPII].[V_HistoricoProcedimientos_Depurado]\", cnxn)\n",
    "\n",
    "\n",
    "# ## Se exportan a un parquet los procesos depurados de SECOP II\n",
    "# SECOP_II_Procesos_Depurado.to_parquet(ruta_Large+\"SECOP_II_Procesos_Depurado.parquet\",engine='pyarrow')\n",
    "\n",
    "# #\n",
    "# ### cerramos la conexión\n",
    "# cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECOP_II_Contratos = pd.read_sql_query(\"SELECT * FROM [CCE_Reports].[SECOPII].[HistoricoContratos]\", cnxn_reports)\n",
    "SECOP_II_Contratos_Depurado = pd.read_sql_query(\"SELECT  * FROM [CCE_Reports].[SECOPII].[V_HistoricoContratos_Depurado]\", cnxn_reports)\n",
    "SECOP_II_Procesos = pd.read_sql_query(\"SELECT  * FROM [CCE_Reports].[SECOPII].[HistoricoProcedimientos_Despliegue]\", cnxn_reports)\n",
    "SECOP_I_Procesos = pd.read_sql_query(\"SELECT * FROM [CCE_Sandbox].[SECOPI].[V_SECOPI]\", cnxn_sandbox)\n",
    "SECOP_I_Contratos = pd.read_sql_query(\"SELECT  * FROM [CCE_Sandbox].[SECOPI].[VWAdjudicacionesDepuradas]\", cnxn_sandbox)\n",
    "\n",
    "## cnxn es la conexión a la base de datos que se estableció en celdas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ruta_Large es la ruta donde se guardará el archivo parquet\n",
    "\n",
    "ruta_Large=\"/dropbox/Dropbox/CCE-projects/Data/\"\n",
    "SECOP_II_Contratos.to_parquet(ruta_Large+\"SECOP_II_Contratos.parquet\", engine='pyarrow')\n",
    "SECOP_II_Contratos_Depurado.to_parquet(ruta_Large+\"SECOP_II_Contratos_Depurado.parquet\",engine='pyarrow')\n",
    "SECOP_II_Procesos.to_parquet(ruta_Large+\"SECOP_II_Procesos.parquet\",engine='pyarrow')\n",
    "SECOP_I_Procesos.to_parquet(ruta_Large+\"SECOP_I_Procesos.parquet\",engine='pyarrow')\n",
    "SECOP_I_Contratos.to_parquet(ruta_Large+\"SECOP_I_Contratos.parquet\",engine='pyarrow')\n",
    "\n",
    "### cerramos la conexión\n",
    "cnxn_reports.close()\n",
    "cnxn_sandbox.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado en DataLake\n",
    "\n",
    "Usamos un token SAS (Shared Access Signature) para acceder al DataLake. Este token se genera en el portal de Azure, en la cuenta de almacenamiento, en la sección de \"Shared access signature\". El token se genera con permisos de lectura y escritura, y con una duración de 3 meses.\n",
    "\n",
    "Un token SAS es una cadena de caracteres que incluye una firma criptográfica generada por el servidor y que contiene información sobre los permisos y la duración del acceso autorizado. Al generar un token SAS, puedes especificar qué acciones se permiten (por ejemplo, leer, escribir, eliminar), qué recursos se pueden acceder y durante cuánto tiempo el token será válido.\n",
    "\n",
    "El uso de tokens SAS te permite delegar el acceso a los recursos de Azure Storage sin compartir tus claves de acceso privadas. Esto proporciona un nivel adicional de seguridad y control sobre tus datos almacenados en Azure.\n",
    "\n",
    "Para generar un token SAS, puedes utilizar las bibliotecas de desarrollo de Azure Storage, la interfaz de línea de comandos de Azure (Azure CLI) o el Portal de Azure. Una vez generado el token SAS, puedes compartirlo con terceros para que puedan acceder a los recursos de Azure Storage durante el período y con los permisos especificados.\n",
    "\n",
    "Es importante tener en cuenta que los tokens SAS son válidos solo durante el período de tiempo especificado y tienen las restricciones y permisos definidos al generarse. Por lo tanto, es fundamental configurar adecuadamente los permisos y la duración del token SAS para garantizar la seguridad y el acceso adecuado a tus datos almacenados en Azure Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "token_sas=os.getenv(\"token_sas\")\n",
    "url_blob=os.getenv(\"url_blob\")\n",
    "\n",
    "### Conectamos con el DataLake\n",
    "# Conexión a la cuenta de almacenamiento de Azure utilizando el token SAS\n",
    "connect_str = f\"BlobEndpoint={url_blob};SharedAccessSignature={token_sas}\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencia al contenedor donde se encuentran los archivos\n",
    "container_name_ent = \"datosemae/bronze/SECOP_ReportsVersion\"\n",
    "container_client_ent = blob_service_client.get_container_client(container_name_ent)\n",
    "\n",
    "# Nombre del archivo a leer\n",
    "blob_name_ent_secop_ii_con = \"SECOP_II_Contratos.parquet\"\n",
    "blob_name_ent_secop_ii_con_dep = \"SECOP_II_Contratos_Depurado.parquet\"\n",
    "blob_name_ent_secop_ii_pro = \"SECOP_II_Procesos.parquet\"\n",
    "blob_name_ent_secop_i_con = \"SECOP_I_Contratos.parquet\"\n",
    "blob_name_ent_secop_i_pro = \"SECOP_I_Procesos.parquet\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos usando Python desde la ruta DATA\n",
    "\n",
    "Cargamos la base desde la ruta DATA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ruta_Large=\"/dropbox/Dropbox/CCE-projects/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la ruta de los archivos\n",
    "ruta_ent_secop_ii_con = ruta_Large+\"/SECOP_II_Contratos.parquet\"\n",
    "ruta_ent_secop_ii_con_dep = ruta_Large+\"/SECOP_II_Contratos_Depurado.parquet\"\n",
    "ruta_ent_secop_ii_pro = ruta_Large+\"/SECOP_II_Procesos.parquet\"\n",
    "ruta_ent_secop_i_con = ruta_Large+\"/SECOP_I_Contratos.parquet\"\n",
    "ruta_ent_secop_i_pro = ruta_Large+\"/SECOP_I_Procesos.parquet\"\n",
    "\n",
    "\n",
    "## Nos conectamos al cliente de Azure\n",
    "\n",
    "blob_client_secop_ii_con = blob_service_client.get_blob_client(container=container_name_ent, blob=blob_name_ent_secop_ii_con)\n",
    "blob_client_secop_ii_con_dep = blob_service_client.get_blob_client(container=container_name_ent, blob=blob_name_ent_secop_ii_con_dep)\n",
    "blob_client_secop_ii_pro = blob_service_client.get_blob_client(container=container_name_ent, blob=blob_name_ent_secop_ii_pro)\n",
    "blob_client_secop_i_con = blob_service_client.get_blob_client(container=container_name_ent, blob=blob_name_ent_secop_i_con)\n",
    "blob_client_secop_i_pro = blob_service_client.get_blob_client(container=container_name_ent, blob=blob_name_ent_secop_i_pro)\n",
    "\n",
    "### Cargamos los archivos al DataLake\n",
    "\n",
    "with open(ruta_ent_secop_ii_con, \"rb\") as data:\n",
    "    blob_client_secop_ii_con.upload_blob(data)\n",
    "\n",
    "with open(ruta_ent_secop_ii_con_dep, \"rb\") as data:\n",
    "    blob_client_secop_ii_con_dep.upload_blob(data)\n",
    "\n",
    "with open(ruta_ent_secop_ii_pro, \"rb\") as data:\n",
    "    blob_client_secop_ii_pro.upload_blob(data)\n",
    "\n",
    "with open(ruta_ent_secop_i_con, \"rb\") as data:\n",
    "    blob_client_secop_i_con.upload_blob(data)\n",
    "\n",
    "with open(ruta_ent_secop_i_pro, \"rb\") as data:\n",
    "    blob_client_secop_i_pro.upload_blob(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "En este ejemplo se mostró como usando python se puede acceder a los datos de la bodega y cargarlos en el DataLake. Vale la pena resaltar que puede ser más provechoso usar Azure CLI para realizar estas tareas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
